一、项目目标
选择以下三个数学问题中的一个，运用O1范式训练大型语言模型（LLM）来解决：
Option 1：24点
24点是一种经典的数学游戏，给定4个数字，通过加、减、乘、除运算（每个数字只能使用一次，可使用括号改变运算顺序），使得最终结果为24。例如给定数字3、3、8、8，可以通过（8÷（3 - 8÷3））=24来得到答案。
Option 2：20位整数内的加减乘除混合运算
例如计算（472397492374 - 4732948）× 473294792 /（4334 - 439），需要对20位以内的整数进行复杂的加减乘除混合运算，考察模型对大数字运算的准确性和逻辑推理能力。
Option 3：数独
数独是一种以数字为元素的逻辑拼图游戏，给定一个9×9的网格，其中部分格子已经填入数字，需要根据规则（每行、每列以及每个3×3的小宫格内数字1 - 9均只能出现一次）将剩余空格填满。例如：
5 3 _ | _ 7 _ | _ _ _
6 _ _ | 1 9 5 | _ _ _
_ 9 8 | _ _ _ | _ 6 _
------+-------+------
8 _ _ | _ 6 _ | _ _ 3
4 _ _ | 8 _ 3 | _ _ 1
7 _ _ | _ 2 _ | _ _ 6
------+-------+------
_ 6 _ | _ _ _ | 2 8 _
_ _ _ | 4 1 9 | _ _ 5
_ _ _ | _ 8 _ | _ 7 9
需要将空白格子填满，使得整个数独满足规则。
二、O1范式介绍
O1范式是一种特定的输入输出模式，即对于输入的问题（question），模型输出一个很长的思考过程（long chain of thought，简称long cot），最终给出答案（answer）。例如对于一个数学问题，模型不仅要给出最终结果，还要详细展示其推理、计算、尝试等过程，就像人类在解决复杂问题时的思维过程一样。
三、时间安排
整个项目的时间为周末两天，需要在这两天内完成数据准备、模型训练、效果评估等全部工作。
四、方法步骤
4.1 数据准备
构造一批带有long cot的X问题数据，用于后续的有监督微调（SFT）训练。X问题即所选的数学问题类型。
Long CoT构造方法
由于所选的数学问题都可以用传统的搜索算法求解，例如A*算法。可以利用A*算法来求解问题，然后将整个搜索过程转化为串行的字符串形式，包括回溯到之前的节点等操作（类似于人类在思考过程中的反思），从而合成大量long cot数据。这样生成的long cot可以包含详细的步骤、尝试的路径、排除的方案等内容，使模型能够学习到完整的解题思路。context长度可以达到4k甚至更多，以容纳丰富的思考过程。
也可以考虑其他创新的方法来合成高质量的long cot数据，以更好地训练模型。
4.2 模型训练
使用小的开源模型，如Qwen - 2.5 - 0.5B进行实验。可以借助开源的训练代码，如llama - factory来进行训练。先进行SFT训练，如果有时间，还可以接着用强化学习（RL）或拒绝采样（Rejection sampling）等方法来进一步优化模型。
五、项目要求
5.1 实验报告
给出完整的实验报告，要求图文并茂，详细记录实验的各个环节，包括数据准备、模型训练过程、效果评估等，通过图表、图片等方式直观展示关键信息，使读者能够清晰地了解整个项目的实施情况。
5.2 数据集
提供训练集（train set）和测试集（test set），测试集不少于100题，且题目难度均匀分布，确保训练集和测试集之间没有重叠，以保证实验结果的有效性。
5.3 过程数据与代码
提供完整的实验过程数据，包括数据集、训练过程中的各种参数设置、中间结果等，以及完整的代码实现，方便他人复现和验证实验。
5.4 加分项
对比不同long cot长度下的模型性能（perf），绘制时间 - 计算量曲线（横轴为token数，纵轴为perf），分析long cot长度对模型性能的影响，为后续的研究和应用提供参考。例如：
[图片]

5.5 面试环节	
项目完成提交后，经过评审通过后可以进入面试环节，有10分钟的时间进行介绍，向面试官展示项目的成果、亮点以及自己的收获和思考等。
六、效果目标
在测试集上，模型的准确率（acc）需要超过50%，以证明模型具备一定的解题能力，能够较好地解决所选的数学问题。
七、计算资源
可以使用Google Colab作为计算资源，利用其提供的免费计算能力来完成模型训练等计算任务，但需要注意合理安排时间和资源，确保在规定时间内完成项目。

Reference
 Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought https://arxiv.org/abs/2501.04682 
